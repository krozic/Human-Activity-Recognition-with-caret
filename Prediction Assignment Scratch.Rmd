---
title: "R Notebook"
output:
  html_document:
    df_print: paged
---

```{r, cache = T}
knitr::opts_chunk$set()
```

```{r}
library(data.table)
library(tidyr)
library(dplyr)
library(ggplot2)
library(caret)
library(rattle)
#library(randomForest)
library(tictoc)
```

http://groupware.les.inf.puc-rio.br/har


Here we will use the `caret` package due to the uniform syntax of the functions between algorithms. 

```{r, cache = TRUE}
if(!file.exists('./data')){dir.create('./data')}
fileUrl1 <- 'https://d396qusza40orc.cloudfront.net/predmachlearn/pml-training.csv'
fileUrl2 <- 'https://d396qusza40orc.cloudfront.net/predmachlearn/pml-testing.csv'
if(!file.exists('./data/pml-training.csv')){download.file(fileUrl1, './data/pml-training.csv')}
if(!file.exists('./data/pml-testing.csv')){download.file(fileUrl2, './data/pml-testing.csv')}
```

```{r, cache = TRUE}
pml_training <- read.csv('./data/pml-training.csv', na.strings=c('', 'NA', '#DIV/0!'))
pml_testing <- read.csv('./data/pml-testing.csv', na.strings = c('', 'NA', '#DIV/0!'))
```

Check integrity of data: 

```{r, eval = FALSE, echo = FALSE}
n <- data.frame(columns = names(pml_training))
n[,'values'] <- NA
n[,'percent'] <- NA

#n <- data.frame(columns = names(pml_training), 
#                values = vector(mode = "double", 
#                                length = ncol(pml_training))))
for (i in 1:ncol(pml_training)) {
        n$values[i] <- sum(!is.na(pml_training[i]))
        n$percent[i] <- (n$values[i]/nrow(pml_training))*100
}

#OR
n[,'values'] <- apply(pml_training, 2, function(x)sum(!is.na(x)))
n[,'percent'] <- apply(pml_training, 2, 
                       function(x)sum(!is.na(x))/length(x))

#OR

colSums(!is.na(pml_training))
colMeans(!is.na(pml_training))*100

#ALSO
apply(!is.na(pml_training), 2, sum)
```

```{r}
as.data.table(table(colMeans(!is.na(pml_training))*100))
```
Many variables have < 2% useable data. Additionally, some variables are not meaningful to the excercise being performed (Ex. user_name, timestamps, window). These are removed 


```{r}
badCols <- as.vector(colMeans(!is.na(pml_training))*100<5)
pml_training_data <- pml_training[,!badCols]
pml_training_data <- pml_training_data[,-c(1:7)]
pml_testing_data <- pml_testing[,!badCols]
pml_testing_data <- pml_testing_data[,-c(1:7)]
```

```{r}
set.seed(13415)
inTrain = createDataPartition(pml_training_data$classe, 
                              p = 0.6, list = F)
training <- pml_training_data[inTrain,]
testing <- pml_training_data[-inTrain,]
```

PreProcessing:

Should we center and scale the variables? Many are 
```{r}
#create preprocessing object with pml_training_train
preObj <- preProcess(training[,-53], method = c('center', 'scale'))
#apply preprocessing object to pml_training_train
trainingCS <- predict(preObj, training[,-53])
trainingCS <- cbind(trainingCS, classe = training$classe)
testingCS <- predict(preObj, testing[,-53])
testingCS <- cbind(testingCS, classe = testing$classe)
```

```{r}
#modelFit <- train(classe~., data = pml_training_trainS, method = 'glm')
```



Review of process:

The options available:

```{r}
# args(train.default)
```


We are using a categorical outcome to predict with, therefore our metrics should be Accuracy(fraction correct) or kappa (measure of concordance). (not RMSE or R^2)


trainControl allows much more precise control.

```{r}
# args(trainControl)
```

## Explore a bit

```{r}
# featurePlot(x = training[,-classe], 
#             y = training$classe,
#             plot = 'pairs')
```

```{r}
# plot(training[,9], training[,1])
```

## Testing Different Models

Here we will test a number of models:

1. Decision Tree
2. Bootstrap Aggregated Tree (bagging)
3. Random Forest
4. Boosted Model

Cross validation can easily be performed with each algorithm using the `trainControl()` function. For this dataset we will use k-fold cross validation. This involves splitting the dataset into k-subsets, then removing a subset while training on the remainder for each subset.I've chosen to perform 5-fold cross validation here.

```{r}
trainC <- trainControl(method = 'cv', number = 5)
```

### 1. Decision Tree

Decision trees estimate a probability per node $m$ for each classication $k$:

$$
\hat{P}_{mk} = \frac{1}{N_m}\sum_{i:x_j\in R_m} 1(y_i = k)
$$
- Here node $m$ represents region $R_m$, with $N_m$ observations.
- This describes the probability for class $k$ in leaf $m$ 
        - In other words, the number of times class $k$ appears in leaf $m$
        

```{r}
start <- Sys.time()
modFitTree <- train(classe ~ ., 
                    method = 'rpart', 
                    data = trainingCS, 
                    trControl = trainC)
Treetime <- Sys.time() - start
```

```{r}
Treetime
```

```{r}
# print(modFit$finalModel)
# plot(modFit$finalModel, uniform = T,
#      main = 'Classification Tree')
# text(modFit$finalModel, use.n = T, all = T, cex = 0.8)
```

```{r}
fancyRpartPlot(modFitTree$finalModel)
```

```{r}
predTree <- predict(modFitTree, newdata = testingCS)
cmTree <- confusionMatrix(predTree, factor(testingCS$classe))
```

```{r}
cmTree
```

It's no surprise that the testing data has such a low accuracy, considering there is no 'D' in the prediction model.

### 2. Bootstrap Aggregated Tree (bagging)

This model uses the bootstrap principle to build 25 tree models from separated subsets of the train data, then constructs a final aggregated model which should be more accurate.

```{r}
start <- Sys.time()
modFitTB <- train(classe ~ ., 
                  method = 'treebag', 
                  data = trainingCS, 
                  trControl = trainC)
TBtime <- Sys.time() - start
```

```{r}
TBtime
```

```{r}
predTB <- predict(modFitTB, newdata = testingCS)
cmTB <- confusionMatrix(predTB, factor(testingCS$classe))
```

```{r}
cmTB
```

### 3. Random Forest

Random forest not only bootstraps over observations for each tree model, but also bootstraps which variables will be considered when determining the probability at each node.

$$
p(c|v) = \frac{1}{T}\sum^{T}_{t}p_t(c|v)
$$

t = tree
T = number of trees
c = outcome
v = observation

$p_t$ is the probability of an outcome $c$ to occur given an observation $v$ for a given tree $t$.
This probability is averaged over the total number of trees $T$.

```{r}
start <- Sys.time()
modFitRF <- train(classe ~ .,
                  method = 'rf',
                  ntrees = 25,
                  data = trainingCS,
                  trControl = trainC)
RFtime <- Sys.time() - start
```

```{r}
RFtime
```


```{r}
# getTree(modFitRF$finalModel, k = 2)
```

Visualizing class 'centers'

```{r}
# test <- classCenter(trainingCS[,c(3,4)], trainingCS$classe, modFitRF$finalModel$prox)
# test <- as.data.frame(test)
# test$classe <- rownames(test)
``` 

```{r}
predRF <- predict(modFitRF, newdata = testingCS)
cmTB <- confusionMatrix(predRF, factor(testingCS$classe))
```

```{r}
cmTB
```



### 4. Boosting with Trees (gbm)

- Similar to 'Bagging' in the sense that we are aggregating a large number of classifiers to build a stronger predictor
- Boosting adjusts the weight of an observation based on the last classification:
        - If the observation $h_t(x)$ in question was previously classified incorrectly, the weight $\alpha_t$ of the observation will be increased.

$$
f(x) = sgn\left(\sum_{t = 1}^T\alpha_th_t(x)\right)
$$

```{r}
start <- Sys.time()
modFitGBM <- train(classe ~ .,
                   method = 'gbm',
                   data = trainingCS,
                   trControl = trainC)
GBMtime <- Sys.time() - start
```

```{r}
GBMtime
```

```{r}
predGBM <- predict(modFitGBM, newdata = testingCS)
cmGBM <- confusionMatrix(predGBM, factor(testingCS$classe))
```

```{r}
cmGBM
```

## Interpreting Model Effectiveness

Confusion Matrix's

```{r}

```



## Conclusion

If this model were used for real-time prediction PCA might help reduce the time required while maintaining 

If this model were being used with a far larger dataset or in real time, it may be useful to employ PCA to reduce the size of the data while maintaining the 






to do:
How we used cv
  done
what is the out of sample error
  This is the error done on the 'test' runs I have

Then I will have the final 
