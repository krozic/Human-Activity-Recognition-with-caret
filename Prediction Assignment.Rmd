---
title: "Human Activity Recognition with Machine Learning"
output:
  html_document:
    df_print: paged
    toc: true
    toc_depth: 3
---

```{r, cache = T, echo = F}
knitr::opts_chunk$set()
```

```{r, message = F, warning = F}
library(data.table)
library(tidyr)
library(dplyr)
library(ggplot2)
library(caret)
library(rattle)
library(tictoc)
library(AppliedPredictiveModeling)
library(kableExtra)
```

The [data](http://groupware.les.inf.puc-rio.br/har) for this project was sourced from the Pontifical Catholic University of Rio de Janeiro (PUC-Rio).$^1$

This project employs the `caret` package in R to build machine learning models that can predict which exercise an individual is performing from wearable sensor data. Sensor data was collected on 4 healthy subjects performing 8 hours of activities. The activities were categorized into 5 classes (sitting-down, standing-up, standing, walking, and sitting).

```{r}
if(!file.exists('./data')){dir.create('./data')}
fileUrl1 <- 'https://d396qusza40orc.cloudfront.net/predmachlearn/pml-training.csv'
fileUrl2 <- 'https://d396qusza40orc.cloudfront.net/predmachlearn/pml-testing.csv'
if(!file.exists('./data/pml-training.csv')){download.file(fileUrl1, './data/pml-training.csv')}
if(!file.exists('./data/pml-testing.csv')){download.file(fileUrl2, './data/pml-testing.csv')}
```

```{r}
pml_training <- read.csv('./data/pml-training.csv', na.strings=c('', 'NA', '#DIV/0!'))
pml_testing <- read.csv('./data/pml-testing.csv', na.strings = c('', 'NA', '#DIV/0!'))
```

```{r}
print(names(pml_training), quote = F)
```

The data contains 160 variables. One of these variables represents the outcome, $classe, while the rest may potentially serve as predictor variables. First I check the integrity of the data, then the data can be prepared for model creation.

## Preparing the Data

### Check integrity of data: 

Making a contingency table based on the percent of usable data in each variable, I can quickly determine which columns are useful and which are not.

```{r, echo = T, eval = F}
as.data.table(table(colMeans(!is.na(pml_training))*100)) %>%
        kbl(col.names = c('% Useable Data', '# of Variables')) %>% 
        kable_styling(bootstrap_options = c('striped',
                                            'hover', 
                                            'condensed', 
                                            'responsive'), 
                      full_width = F)
```

**Table 1.** Table showing the count of variables containing the shown amount of useable data.

```{r, echo = F, eval = T}
as.data.table(table(colMeans(!is.na(pml_training))*100)) %>%
        kbl(col.names = c('% Useable Data', '# of Variables')) %>% 
        kable_styling(bootstrap_options = c('striped',
                                            'hover', 
                                            'condensed', 
                                            'responsive'), 
                      full_width = F)
```

Many variables have < 2% useable data. Additionally, some variables are not meaningful to the excercise being performed (Ex. user_name, timestamps, window). These are removed before any model training is performed.

Additionally, the outcome variable `$classe` is coerced to a factor.

### Subsetting Data and Partitioning Training/Testing Sets

```{r}
pml_training$classe <- factor(pml_training$classe)
badCols <- as.vector(colMeans(!is.na(pml_training))*100<5)
pml_training_data <- pml_training[,!badCols]
pml_training_data <- pml_training_data[,-c(1:7)]
pml_testing_data <- pml_testing[,!badCols]
pml_testing_data <- pml_testing_data[,-c(1:7)]
```

```{r}
set.seed(13415)
inTrain = createDataPartition(pml_training_data$classe, 
                              p = 0.6, list = F)
training <- pml_training_data[inTrain,]
testing <- pml_training_data[-inTrain,]
```

`summary()` was used to compare variables in the data and it was found that the distribution of each variable differed greatly from the others. However, feature scaling is not necessary for decision tree type models since they are not sensitive to variance between variables (it will only reduce readability of the data and models).

## Testing Different Models

Since I am predicting a categorical outcome, I used tree predictors and the metric to determine performance was Accuracy (fraction correct), not RMSE or $R^2$. Kappa (measure of concordance) can also be used, however the outcome scenarios are not significantly skewed enough to require normalization of this metric:

```{r}
prop.table(table(training$classe))
```

For model creation I used the `caret` package due to the uniform syntax of the functions between algorithms. (Week 3 of Data Science from JHU (Coursera) describes decision tree models, and I chose to use the models described there.)

I tested four different models:

1. Decision Tree
2. Bootstrap Aggregated Tree (bagging)
3. Random Forest
4. Boosted Model

Cross validation can easily be performed with each algorithm using the `trainControl()` function. For this dataset I used k-fold cross validation. This involves splitting the observations into k-subsets, then removing a subset while training on the remainder for each subset. I've chosen to perform 5-fold cross validation here.

```{r}
trainC <- trainControl(method = 'cv', number = 5)
```

For each model, I trained the model on the `training` data, predicted outcomes on the `testing` data, then analyzed the *out of sample accuracy* of the predictions by running a confusion matrix on predictions and testing outcomes with `confusionMatrix()`. I also measured the time for each model to train using `Sys.time()`.

### 1. Decision Tree

Decision trees estimate a probability per node $m$ for each classification $k$:

$$
\hat{P}_{mk} = \frac{1}{N_m}\sum_{i:x_j\in R_m} 1(y_i = k)
$$

- Here node $m$ represents region $R_m$, with $N_m$ observations.
- This describes the probability for class $k$ in leaf $m$ 
        - In other words, the number of times class $k$ appears in leaf $m$
        

```{r}
set.seed(13415)
start <- Sys.time()
modFitTree <- train(classe ~ ., 
                    method = 'rpart', 
                    data = training, 
                    trControl = trainC)
Treetime <- Sys.time() - start
```

```{r}
predTree <- predict(modFitTree, newdata = testing)
cmTree <- confusionMatrix(predTree, factor(testing$classe))
cmTree
Treetime
```

It is no surprise that the testing data has such a low out of sample accuracy (49.4%), considering there is no 'D' in the prediction model.

### 2. Bootstrap Aggregated Tree (bagging)

This model uses the bootstrap principle to build 25 tree models from separated subsets of the train data, then constructs a final aggregated model which should be more accurate.

```{r}
set.seed(13415)
start <- Sys.time()
modFitTB <- train(classe ~ ., 
                  method = 'treebag', 
                  data = training, 
                  trControl = trainC)
TBtime <- Sys.time() - start
```

```{r}
predTB <- predict(modFitTB, newdata = testing)
cmTB <- confusionMatrix(predTB, factor(testing$classe))
cmTB
TBtime
```

The 'D' outcome has now been utilized and the out of sample accuracy has increased to 98.1% with a time increase of ~28 seconds

### 3. Random Forest

Random forest not only bootstraps over observations for each tree model, but also bootstraps which variables will be considered when determining the probability at each node.

$$
p(c|v) = \frac{1}{T}\sum^{T}_{t}p_t(c|v)
$$

$t$ = tree, 
$T$ = number of trees, 
$c$ = outcome, 
$v$ = observation 

- $p_t$ is the probability of an outcome $c$ to occur given an observation $v$ for a given tree $t$.
- This probability is averaged over the total number of trees $T$.

```{r}
set.seed(13415)
start <- Sys.time()
modFitRF <- train(classe ~ .,
                  method = 'rf',
                  ntrees = 25,
                  data = training,
                  trControl = trainC)
RFtime <- Sys.time() - start
```

```{r}
predRF <- predict(modFitRF, newdata = testing)
cmTB <- confusionMatrix(predRF, factor(testing$classe))
cmTB
RFtime
```

The random forest model provides a further increase in out of sample accuracy (98.8%) with a time increase of ~4 minutes.

### 4. Boosting with Trees (gbm)

Boosting with trees is similar to 'Bagging' in the sense that we are aggregating a large number of classifiers to build a stronger predictor. However, boosting adjusts the weight of an observation based on the last classification:

$$
f(x) = sgn\left(\sum_{t = 1}^T\alpha_th_t(x)\right)
$$

- If the observation $h_t(x)$ in question was previously classified incorrectly, the weight $\alpha_t$ of the observation will be increased.
- The default number of trees for gbm is 100.
        
```{r, results = 'hide'}
set.seed(13415)
start <- Sys.time()
modFitGBM <- train(classe ~ .,
                   method = 'gbm',
                   data = training,
                   trControl = trainC)
GBMtime <- Sys.time() - start
```

```{r}
predGBM <- predict(modFitGBM, newdata = testing)
cmGBM <- confusionMatrix(predGBM, factor(testing$classe))
cmGBM
GBMtime
```

The gbm model resulted in a lower out of sample accuracy than random forest and bagged trees (95.8%) with a final time of ~2.2 minutes. 

## Model Comparison and Interpretation

After taking a look at the out of sample error rates for each of the models, the random forest model `modFitRF` is found to be the best choice with an accuracy of 98.8% and a kappa of 98.5%.

The top 20 most important variables can be viewed with `varImp(modFitRF)`:

```{r}
varImp(modFitRF)
```

It may be useful to determine which variables more strongly contribute to a successful prediction. This information can be used to prune the sensor apparatus to save cost.

For example, there are no 'gyros' variables in the top 20 important variables, indicating that this component of the sensor array may not be necessary for successful prediction of exercise type.

Visualizing important variables in the chosen model:

```{r}
imp_var <- as.data.frame((varImp(modFitRF)[1])$importance)
imp_var <- rownames_to_column(imp_var) %>% arrange(desc(Overall))
imp_data <- training %>% select(imp_var[1:5, 1])
```

```{r}
transparentTheme(trans = 0.3)
featurePlot(x = imp_data, 
            y = factor(training$classe),
            plot = 'box',
            scales = list(x = list(relation = 'free'),
                          y = list(relation = 'free')),
            auto.key = list(columns = 5)
            )
```

**Figure 1.** Box plots of the top 5 predictors for the random forest model.

```{r}
transparentTheme(trans = 0.3)
featurePlot(x = imp_data, 
            y = factor(training$classe),
            plot = 'density',
            scales = list(x = list(relation = 'free'),
                          y = list(relation = 'free')),
            pch = '|',
            auto.key = list(columns = 5)
            )
```

**Figure 2.** Density plots of the top 5 predictors for the random forest model.

The box plot reveals noticeable changes between exercise types in the `$pitch_forearm` variable, however there isn't much obvious difference between exercise types for the other variables. The density plots 

## Conclusion

In conclusion, the random forest model was found to perform the best with an out of sample error rate of 1.2%. The same K-fold cross validation was used to optimize these models, however further optimization of tree numbers for the bagged, random forest, and gbm models could result in higher accuracy using a grid search type cross validation. This would considerably increase the model creation time (hours per model with my hardware), so I chose not to pursue this optimization at this time.

## References

1. Ugulino, W.; Cardador, D.; Vega, K.; Velloso, E.; Milidiu, R.; Fuks, H. Wearable Computing: Accelerometers' Data Classification of Body Postures and Movements. Proceedings of 21st Brazilian Symposium on Artificial Intelligence. Advances in Artificial Intelligence - SBIA 2012. In: Lecture Notes in Computer Science. , pp. 52-61. Curitiba, PR: Springer Berlin / Heidelberg, 2012. ISBN 978-3-642-34458-9. DOI: 10.1007/978-3-642-34459-6_6.